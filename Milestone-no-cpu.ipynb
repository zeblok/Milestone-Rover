{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "\n",
    "client = Minio('130.245.177.209:9000',               \n",
    "               access_key='<key>',\n",
    "               secret_key='<key>',\n",
    "               secure=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add U-Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u100_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/u100.csv')\n",
    "u101_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/u101.csv')\n",
    "u104_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/u104.csv')\n",
    "u200_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/u200.csv')\n",
    "u175_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/u175.csv')\n",
    "u141_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/u141.csv')\n",
    "u108_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/u108.csv')\n",
    "u178_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/u178.csv')\n",
    "u280_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/u280.csv')\n",
    "u210_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/u210.csv')\n",
    "u780_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/u780_NAV-Movement.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add L-Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l220_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/l220.csv')\n",
    "l222_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/l222.csv')\n",
    "l225_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/l225.csv')\n",
    "l235_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/l235.csv')\n",
    "l295_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/l295.csv')\n",
    "l315_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/l315.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add P-Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p200_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/p200.csv')\n",
    "p201_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/p201.csv')\n",
    "p202_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/p202.csv')\n",
    "p204_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/p204.csv')\n",
    "p205_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/p205.csv')\n",
    "p206_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/p206.csv')\n",
    "p207_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/p207.csv')\n",
    "p208_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/p208.csv')\n",
    "p209_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/p209.csv')\n",
    "p210_obj = client.get_object('datalake', 'epapenhausen/Milestone/pdavies/p210.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "u100 = pd.read_csv(u100_obj)\n",
    "u101 = pd.read_csv(u101_obj)\n",
    "u104 = pd.read_csv(u104_obj)\n",
    "u200 = pd.read_csv(u200_obj)\n",
    "u175 = pd.read_csv(u175_obj)\n",
    "u141 = pd.read_csv(u141_obj)\n",
    "u108 = pd.read_csv(u108_obj)\n",
    "u178 = pd.read_csv(u178_obj)\n",
    "u280 = pd.read_csv(u280_obj)\n",
    "u210 = pd.read_csv(u210_obj)\n",
    "u780 = pd.read_csv(u780_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p200 = pd.read_csv(p200_obj)\n",
    "p201 = pd.read_csv(p201_obj)\n",
    "p202 = pd.read_csv(p202_obj)\n",
    "p204 = pd.read_csv(p204_obj)\n",
    "p205 = pd.read_csv(p205_obj)\n",
    "p206 = pd.read_csv(p206_obj)\n",
    "p207 = pd.read_csv(p207_obj)\n",
    "p208 = pd.read_csv(p208_obj)\n",
    "p209 = pd.read_csv(p209_obj)\n",
    "p210 = pd.read_csv(p210_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l220 = pd.read_csv(l220_obj)\n",
    "l222 = pd.read_csv(l222_obj)\n",
    "l225 = pd.read_csv(l225_obj)\n",
    "l235 = pd.read_csv(l235_obj)\n",
    "l295 = pd.read_csv(l295_obj)\n",
    "l315 = pd.read_csv(l315_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compound unique keys: (Entity Code, Effective Date, Run_ID, Price Run)\n",
    "def sanitize(table, run_type='Oversight', status=None):\n",
    "    filt = (table['price_run_type']==run_type)\n",
    "    if status is not None:\n",
    "        filt = filt&(table['p_status']==status)\n",
    "    \n",
    "    table_san = table[filt]\n",
    "    drop = [c for c in table_san.columns if table_san[c].nunique() <= 1]+['pcontrol_id']\n",
    "    return table_san.drop(drop, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u101_san = sanitize(u101, status='V')\n",
    "u100_san = sanitize(u100, status='V')\n",
    "\n",
    "a01 = sanitize(p200)\n",
    "a02 = sanitize(p201)\n",
    "a03 = sanitize(p202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_summary(raw, keys, data_name):\n",
    "    ''' Computes summary for categorical attributes using \"keys\" as a primary key. '''\n",
    "    categories = [c for c in raw.columns[raw.dtypes==object] if c not in keys]\n",
    "    cat_data = {str(cat)+'_'+str(level): [] for cat in categories for level in raw[cat].unique()}\n",
    "\n",
    "    data = {**{k: [] for k in keys}, 'num_'+data_name: [], **cat_data}    \n",
    "    for k, g in raw.groupby(keys): #'pcontrol_code'):\n",
    "        n = g.shape[0]\n",
    "        base_value = g.base_mv_close.sum()\n",
    "        \n",
    "        for cd in cat_data:\n",
    "            data[cd].append(0)\n",
    "                    \n",
    "        for cat in categories:        \n",
    "            #cat_vc = g[cat].value_counts()\n",
    "            cat_vc = g.groupby(cat)['base_mv_close'].sum()\n",
    "            for level, value in cat_vc.iteritems():\n",
    "                index = str(cat)+'_'+str(level)                     \n",
    "                data[index][-1] = value / base_value\n",
    "                                \n",
    "        for i, ky in enumerate(keys):\n",
    "            data[ky].append(k[i])\n",
    "        \n",
    "        data['num_'+data_name].append(n)          \n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['pcontrol_code', 'effective_date']\n",
    "u100_data = nested_summary(u100_san, keys, 'assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "primary_key = ['pcontrol_code', 'effective_date']\n",
    "product_data = pd.merge(a01, u101_san, how='left', left_on=primary_key, right_on=primary_key)\n",
    "product_data = pd.merge(product_data, u100_data, how='left', left_on=primary_key, right_on=primary_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify columns to be dropped as they are not relevant to the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop date columns not relevant to the analysis\n",
    "datedrop = ['last_updated_date_x', 'last_updated_date_y', 'run_date_and_time', 'value_date', 'effective_date']\n",
    "\n",
    "# since we will be using the max_tol_lvl to construct the target variable, drop variables that could cause leakage\n",
    "leakdrop = ['max_tol_lvl', 'holding_max_tol_lvl']\n",
    "\n",
    "# drop columns indicating the data source\n",
    "sourcedrop = [c for c in product_data.columns if '_source_' in c]\n",
    "\n",
    "# drop columns relating to internal pcontrol flags\n",
    "pcontroldrop = ['integrity_value', 'pcontrol_code'] + [c for c in product_data.columns if 'batch' in c] + [c for c in product_data.columns if c[:2] == 'c_'] + [c for c in product_data.columns if 'updated_by' in c]\n",
    "\n",
    "drop = datedrop + leakdrop + sourcedrop + pcontroldrop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the Year, Month, and Day from the date columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "value_year = product_data['value_date'].apply(lambda x: float(str(x)[:4]) if not np.isnan(x) else x)\n",
    "value_month = product_data['value_date'].apply(lambda x: float(str(x)[4:6]) if not np.isnan(x) else x)\n",
    "value_day = product_data['value_date'].apply(lambda x: float(str(x)[6:]) if not np.isnan(x) else x)\n",
    "\n",
    "eff_year = product_data['effective_date'].apply(lambda x: float(str(x)[:4]) if not np.isnan(x) else x)\n",
    "eff_month = product_data['effective_date'].apply(lambda x: float(str(x)[4:6]) if not np.isnan(x) else x)\n",
    "eff_day = product_data['effective_date'].apply(lambda x: float(str(x)[6:]) if not np.isnan(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data['value_date_year'] = value_year\n",
    "product_data['value_date_month'] = value_month\n",
    "product_data['value_date_day'] = value_day\n",
    "\n",
    "product_data['effective_date_year'] = eff_year\n",
    "product_data['effective_date_month'] = eff_month\n",
    "product_data['effective_date_day'] = eff_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data['pcontrol_code'].unique()\n",
    "\n",
    "fund_names = u100_san['pcontrol_code'].unique()\n",
    "product_data['fund'] = product_data['pcontrol_code'].apply(lambda x: next((f for f in fund_names if f in x), x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A01 Product Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_context_map.pattern_miner as pm\n",
    "\n",
    "# A tolerance breach has occured if max_tol_lvl > 0\n",
    "data = product_data.copy()\n",
    "data['breached'] = (data['max_tol_lvl']>0).astype(int)\n",
    "\n",
    "# drop the occurrences where the previous nav amount is 0\n",
    "data = data[data['net_nav_amount_prev']!=0]\n",
    "data.drop(drop, axis=1, inplace=True)\n",
    "\n",
    "target = 'breached'\n",
    "out = pm.DataContextMap(data.drop(['fund'], axis=1), target, max_pattern=1000, max_depth=3, mine_type='binary', holdout=0)\n",
    "out.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
